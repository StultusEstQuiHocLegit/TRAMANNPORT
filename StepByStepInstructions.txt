Welcome to Alpha Vantage! Your dedicated access key is: M028U1TYNOXCZPN9

















library(httr)
library(jsonlite)

# Set API key
api_key <- "M028U1TYNOXCZPN9"

# Function to fetch data from Alpha Vantage and save as CSV
fetch_and_save <- function(url, filename) {
  response <- GET(url)
  content <- content(response, as = "text", encoding = "UTF-8")
  data <- fromJSON(content, flatten = TRUE)
  
  if (!is.null(data)) {
    write.csv(data, file = filename, row.names = FALSE)
    message(paste("Saved:", filename))
  } else {
    message("Failed to retrieve data.")
  }
}

# Define API endpoints and filenames
stock_prices_url <- paste0("https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=AAPL&apikey=", api_key)
fetch_and_save(stock_prices_url, "stock_prices.csv")

trading_volume_url <- paste0("https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=AAPL&apikey=", api_key)
fetch_and_save(trading_volume_url, "trading_volume.csv")

news_sentiment_url <- paste0("https://www.alphavantage.co/query?function=NEWS_SENTIMENT&apikey=", api_key)
fetch_and_save(news_sentiment_url, "news_sentiment.csv")

trending_tickers_url <- paste0("https://www.alphavantage.co/query?function=TRENDING_TICKERS&apikey=", api_key)
fetch_and_save(trending_tickers_url, "trending_tickers.csv")




























// ////////////////////////////////////////////////////////////////////////////////////////////////////// i ran this code:

library(data.table)
library(lubridate)

#################################################### 
# Load Robintrack (see OLAT course Empirical Finance)
my_path_to_robintrack <- "./data/popularity_export/"
start_time <- Sys.time()
rh_file_list <- list.files(my_path_to_robintrack)

no_files <- length(rh_file_list)
counter <- 1
datalist <- vector(mode = "list", length = no_files)

for (my_file in rh_file_list) {    
    # read data for one ticker  
    my_ticker <- fread(file.path(my_path_to_robintrack, my_file))    
    
    # create NY timestamp  
    my_ticker[, NY_timestamp := with_tz(timestamp, "America/New_York")]    
    
    # reduce data set to one observation by date  
    my_ticker[, last_timestamp := max(timestamp), by = date(timestamp)]  
    
    short_dt <- my_ticker[last_timestamp == timestamp, .(NY_timestamp, users_holding)]    
    
    # create a new variable that identifies the security  
    short_dt[, ticker := substring(my_file, 1, nchar(my_file) - 4)]    
    
    # append daily data  
    datalist[[counter]] <- short_dt    
    
    # verbose  
    if (counter %% 100 == 0) {
        print(paste(counter, "/", no_files))
    }  
    counter <- counter + 1  
}

rh_daily <- rbindlist(datalist)
end_time <- Sys.time()
print(paste("This code runs", end_time - start_time, "minutes on my machine."))

########################################### 
# Load IPO data
ipo <- fread('./data/SCOOP-Rating-Performance_prepared.csv')

# Correctly load the date column with backticks
ipo[, lub_date := mdy(`Trade Date`)]  # Use backticks for column names with spaces

# Now, check if lub_date was created successfully
if ("lub_date" %in% names(ipo)) {
    ipo[, .N, keyby = year(lub_date)]
    
    ipo[, parsed_px_chng := substr(`1st Day % Px Chng`, 1, nchar(`1st Day % Px Chng`) - 2)]
    ipo[, parsed_px_chng := gsub(',', '.', parsed_px_chng)]
    ipo[, parsed_px_chng := as.numeric(parsed_px_chng)]
    
    # Check if parsed_px_chng is created
    if ("parsed_px_chng" %in% names(ipo)) {
        ipo_summary <- ipo[, .(mean(parsed_px_chng, na.rm = TRUE), .N), keyby = year(lub_date)]
    }

    setnames(ipo, "Symbol", "ticker")

    rh_daily[, date := date(NY_timestamp)]

    # Eliminate duplicates in IPO data
    setorder(ipo, ticker, lub_date)
    ipo[, dup := 1:.N, by = ticker]
    ipo[, max_dup := max(dup), by = ticker] 
    ipo <- ipo[dup == max_dup]
    ipo[, dup := NULL] 
    ipo[, max_dup := NULL] 

    merged_data <- merge(rh_daily, ipo, by = c("ticker"), all.x = TRUE)

    # Save merged data
    fwrite(merged_data, './data/RobintrackAndSCOOP_merged_data.csv')
} else {
    stop("The 'lub_date' column was not created. Check the previous steps for errors.")
}

















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

library(data.table)  # Load data.table package

# Load the merged data
merged_data <- fread('./data/RobintrackAndSCOOP_merged_data.csv')

# Identify rows where all columns after 'date' are empty
filtered_data <- merged_data[!(is.na(`Trade Date`) & is.na(Issuer) & is.na(`Lead/Joint-Lead Managers`) &
                               is.na(`Offer Price`) & is.na(`Opening Price`) & is.na(`1st Day Close`) &
                               is.na(`1st Day % Px Chng`) & is.na(`$ Change Opening`) & is.na(`$ Change Close`) &
                               is.na(`Star Ratings`) & is.na(Performed) & is.na(lub_date) & is.na(parsed_px_chng))]

# Save the cleaned dataset
fwrite(filtered_data, './data/RobintrackAndScoop_merged_data_OnlyCompleteLines.csv')


























// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

# Read the file as a text
file_path <- "./data/RobintrackAndScoop_merged_data_OnlyCompleteLines.csv"
file_content <- readLines(file_path)

# Remove lines that contain only empty columns
cleaned_content <- file_content[!grepl(',"","","","","","","","","","","",,', file_content)]

# Write the cleaned content back to a new file
writeLines(cleaned_content, "RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned.csv")




























// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

see the file: ./test_pytrends.py




















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

// now we are going back to our original idea to just focus on few, representative cases and fetch the data for them manually


library(ggplot2)
library(readr)
library(dplyr)

# Read the CSV file
file_path <- "./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned.csv"
df <- read_csv(file_path)

df <- df %>%
  mutate(across(c(`Offer Price`, `Opening Price`, `1st Day Close`, `$ Change Opening`, `$ Change Close`, `parsed_px_chng`),
                ~ as.numeric(gsub(",", ".", gsub("\\$", "", .)), na.rm = TRUE)))

df$`Offer Price` <- as.numeric(gsub(",", ".", gsub("\\$", "", df$`Offer Price`)))
df$`Opening Price` <- as.numeric(gsub(",", ".", gsub("\\$", "", df$`Opening Price`)))
df$`1st Day Close` <- as.numeric(gsub(",", ".", gsub("\\$", "", df$`1st Day Close`)))
df$`$ Change Opening` <- as.numeric(gsub(",", ".", gsub("\\$", "", df$`$ Change Opening`)))
df$`$ Change Close` <- as.numeric(gsub(",", ".", gsub("\\$", "", df$`$ Change Close`)))
df$parsed_px_chng <- as.numeric(gsub(",", ".", df$parsed_px_chng))



ggplot(df, aes(x = parsed_px_chng)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of 1st Day % Price Change",
       x = "1st Day % Price Change",
       y = "Frequency") +
  theme_minimal()





















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

// now we are compressing the data so that each IPO appears only once


library(dplyr)

# Define file paths
input_file <- "./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned.csv"
output_file <- "./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned_compressed.csv"

# Read CSV file preserving original column names
ipo_data <- read.csv(input_file, stringsAsFactors = FALSE, check.names = FALSE)

# Aggregate the data so each IPO appears only once
ipo_summary <- ipo_data %>%
  group_by(ticker) %>%  # Change grouping if multiple columns define a unique IPO
  summarise(
    avg_users_holding = round(mean(users_holding, na.rm = TRUE), 2),
    NY_timestamp = first(NY_timestamp),
    date = first(date),
    `Trade Date` = first(`Trade Date`),
    Issuer = first(Issuer),
    `Lead/Joint-Lead Managers` = first(`Lead/Joint-Lead Managers`),
    `Offer Price` = first(`Offer Price`),
    `Opening Price` = first(`Opening Price`),
    `1st Day Close` = first(`1st Day Close`),
    `1st Day % Px Chng` = first(`1st Day % Px Chng`),
    `$ Change Opening` = first(`$ Change Opening`),
    `$ Change Close` = first(`$ Change Close`),
    `Star Ratings` = first(`Star Ratings`),
    Performed = first(Performed),
    lub_date = first(lub_date),
    parsed_px_chng = first(parsed_px_chng)
  )

# Save the aggregated data as a new CSV file
write.csv(ipo_summary, file = output_file, row.names = FALSE)

# View the resulting summary
print(ipo_summary)















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

// compare users holding an percentage price change on first day (IPO underpricing)

# Load necessary library. Install it if you haven't already.
if(!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Read the data from the CSV file.
data <- read.csv("./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned_compressed.csv",
                 stringsAsFactors = FALSE)

# If needed, clean/convert the numeric columns.
# For example, if the numbers include commas as thousands separators or decimal commas,
# you may need to remove or replace them. The following are examples:

# Remove commas from avg_users_holding (if they are used as thousands separators)
data$avg_users_holding <- as.numeric(gsub(",", "", data$avg_users_holding))

# Replace comma with dot for parsed_px_chng if necessary (and then convert to numeric)
data$parsed_px_chng <- as.numeric(gsub(",", ".", data$parsed_px_chng))

# Create a scatter plot.
ggplot(data, aes(x = avg_users_holding, y = parsed_px_chng)) +
  geom_point(alpha = 0.6, color = "blue") +
  labs(title = "Comparison of Average Users Holding and IPO Underpricing",
       x = "Average Users Holding",
       y = "First Day % Price Change (IPO Underpricing)") +
  theme_minimal() +
  geom_smooth(method = "lm", se = FALSE, color = "red")  # Optional: add a linear trend line














// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

# Fit a linear model
lm_model <- lm(parsed_px_chng ~ avg_users_holding, data = data)

# Extract coefficients: intercept and slope
coeffs <- coef(lm_model)
cat("Regression Formula: parsed_px_chng =", coeffs[1], "+", coeffs[2], "* avg_users_holding\n")



// ////////////////////////////////////////////////// with this result:

// Regression Formula: parsed_px_chng = 15.84487 + 0.000170225 * avg_users_holding



















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

// to look more into the main part and exclude the outliers

# Load necessary library
if(!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Read the data from the CSV file
data <- read.csv("./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned_compressed.csv",
                 stringsAsFactors = FALSE)

# Data cleaning: Remove commas from numeric columns if necessary
data$avg_users_holding <- as.numeric(gsub(",", "", data$avg_users_holding))
data$parsed_px_chng <- as.numeric(gsub(",", ".", data$parsed_px_chng))

# Define a threshold to exclude extreme outliers
threshold <- quantile(data$avg_users_holding, 0.95)  # Exclude the top 5% of values

# Filter data to focus on the main part
filtered_data <- data[data$avg_users_holding <= threshold, ]

# Create a scatter plot with filtered data
ggplot(filtered_data, aes(x = avg_users_holding, y = parsed_px_chng)) +
  geom_point(alpha = 0.6, color = "blue") +
  labs(title = "Comparison of Average Users Holding and IPO Underpricing (Filtered)",
       x = "Average Users Holding",
       y = "First Day % Price Change (IPO Underpricing)") +
  theme_minimal() +
  geom_smooth(method = "lm", se = FALSE, color = "red")  # Add trend line
















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

# Fit a linear model using the filtered data
lm_model <- lm(parsed_px_chng ~ avg_users_holding, data = filtered_data)

# Extract coefficients: intercept and slope
coeffs <- coef(lm_model)
cat("Regression Formula: parsed_px_chng =", coeffs[1], "+", coeffs[2], "* avg_users_holding\n")



// ////////////////////////////////////////////////// with this result:

// Regression Formula: parsed_px_chng = 13.19497 + 0.002161328 * avg_users_holding


















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

// for getting some other interesting graphics


// histogram of IPO underpricing

ggplot(data, aes(x = parsed_px_chng)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of IPO Underpricing (First Day % Change)",
       x = "First Day % Price Change",
       y = "Count") +
  theme_minimal()


// boxplot for users holding vs. IPO performance

ggplot(data, aes(x = as.factor(parsed_px_chng > 0), y = avg_users_holding)) +
  geom_boxplot(fill = c("red", "green"), alpha = 0.6) +
  labs(title = "Users Holding for Winning vs. Losing IPOs",
       x = "IPO Performance (0 = Loss, 1 = Gain)",
       y = "Average Users Holding") +
  theme_minimal()


// density plot for users holding for positive vs. negative IPOs

ggplot(data, aes(x = avg_users_holding, fill = as.factor(parsed_px_chng > 0))) +
  geom_density(alpha = 0.5) +
  labs(title = "Density of Users Holding for Profitable vs. Losing IPOs",
       x = "Average Users Holding",
       fill = "IPO Performance (0 = Loss, 1 = Gain)") +
  theme_minimal()


// bar chart for count of IPOs by performance category

data$performance_category <- cut(data$parsed_px_chng, 
                                 breaks = c(-Inf, -5, 0, 5, Inf), 
                                 labels = c("Highly Overpriced", "Slightly Overpriced", "Slightly Underpriced", "Highly Underpriced"))

ggplot(data, aes(x = performance_category)) +
  geom_bar(fill = "steelblue", alpha = 0.7) +
  labs(title = "Count of IPOs by Performance Category",
       x = "IPO Performance Category",
       y = "Count") +
  theme_minimal()


// heatmap of users holding vs. IPO performance

library(ggplot2)

ggplot(data, aes(x = avg_users_holding, y = parsed_px_chng)) +
  geom_bin2d(bins = 20) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Heatmap of Users Holding vs. IPO Underpricing",
       x = "Average Users Holding",
       y = "First Day % Price Change") +
  theme_minimal()


// time series of IPO performance over time

ggplot(data, aes(x = as.Date(date), y = parsed_px_chng)) +
  geom_line(color = "darkgreen") +
  geom_point(alpha = 0.5) +
  labs(title = "IPO Underpricing Over Time",
       x = "IPO Date",
       y = "First Day % Price Change") +
  theme_minimal()


// faceted scatter plot by star rating

ggplot(data, aes(x = avg_users_holding, y = parsed_px_chng)) +
  geom_point(alpha = 0.6, color = "blue") +
  facet_wrap(~ Star.Ratings) +
  labs(title = "Users Holding vs. IPO Underpricing (by Star Rating)",
       x = "Average Users Holding",
       y = "First Day % Price Change") +
  theme_minimal()














  // ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

// we already checked if there is a correlation between users holding und IPO underpricing,
// now do the same for the others columns, like date and star ratings

# Load necessary libraries
library(dplyr)
library(ggplot2)

# Read the data
data <- read.csv("./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned_compressed.csv", stringsAsFactors = FALSE)

# Convert date column to year and check correlation
data$year <- as.numeric(substr(data$date, 1, 4))  # Extract year
correlation_year <- cor(data$avg_users_holding, data$year, use = "complete.obs")

# Convert Star Ratings to numeric
data$Star.Ratings <- as.numeric(data$Star.Ratings)
correlation_star_ratings <- cor(data$avg_users_holding, data$Star.Ratings, use = "complete.obs")

# Print correlations
cat("Correlation with Year:", correlation_year, "\n")
cat("Correlation with Star Ratings:", correlation_star_ratings, "\n")


// ////////////////////////////////////////////////// with this result:

// Correlation with Year: -0.01540572
// Correlation with Star Ratings: 0.163463

// -> both very small -> not so important compared to the effect of IPO underpricing


















// ////////////////////////////////////////////////////////////////////////////////////////////////////// then this one

// getting the top 30 with (most + least) (underpricing + holders)


# Load required library
library(dplyr)

# Read the CSV file (adjust the path if necessary)
data <- read.csv("./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned_compressed.csv", 
                 stringsAsFactors = FALSE)

# Convert columns to numeric if necessary.
# For example, if parsed_px_chng is not numeric, uncomment the next line:
# data$parsed_px_chng <- as.numeric(data$parsed_px_chng)

# Group 1: Top 30 Most Underpricing (largest parsed_px_chng)
most_underpricing <- data %>% 
  arrange(desc(parsed_px_chng)) %>% 
  head(30)

# Group 2: Top 30 Least Underpricing (lowest parsed_px_chng)
least_underpricing <- data %>% 
  arrange(parsed_px_chng) %>% 
  head(30)

# Group 3: Top 30 Most Users Holding (largest avg_users_holding)
most_users <- data %>% 
  arrange(desc(avg_users_holding)) %>% 
  head(30)

# Group 4: Top 30 Least Users Holding (smallest avg_users_holding)
least_users <- data %>% 
  arrange(avg_users_holding) %>% 
  head(30)

# Define the output file name
output_file <- "./data/RobintrackAndScoop_merged_data_OnlyCompleteLines_cleaned_compressed_ExtremestParts.csv"

# Open a connection to the output file
con <- file(output_file, "w")

# Function to write a group with a header and three empty lines afterward
write_group <- function(header, df) {
  writeLines(header, con)
  # Write the data frame (with header) to the file; append = TRUE ensures we add to the file
  write.table(df, file = con, sep = ",", row.names = FALSE, col.names = TRUE, append = TRUE, quote = FALSE)
  # Write three empty lines as separator
  writeLines("\n\n\n", con)
}

# Write each group with a header comment

# Group 1: Most Underpricing
write_group("# Top 30 Most Underpricing", most_underpricing)

# Group 2: Least Underpricing
write_group("# Top 30 Least Underpricing", least_underpricing)

# Group 3: Most Users Holding
write_group("# Top 30 Most Users Holding", most_users)

# Group 4: Least Users Holding
write_group("# Top 30 Least Users Holding", least_users)

# Close the connection
close(con)


    
























